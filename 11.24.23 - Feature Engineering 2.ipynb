{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0af8808-6f7c-4133-bb9f-3af5c18300fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ans 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29179347-8732-4c90-b471-39dba2e66285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Min-max scaling is a common technique used in data preprocessing to scale numerical features to a specific\n",
    "# range, usually between 0 and 1. It's calculated using the formula:\n",
    "    \n",
    "#     X_scaled = (X - X_min)/(X_max - X_min)\n",
    "    \n",
    "# where:\n",
    "# X is the original value of the feature.\n",
    "# X_min is the minimum value of the feature.\n",
    "# X_max is the maximum value of the feature.\n",
    "\n",
    "# This scaling method is particularly useful when the features have different scales and ranges, ensuring that all features\n",
    "# contribute equally to the analysis.\n",
    "\n",
    "# In Python, you can use the MinMaxScaler from the sklearn.preprocessing module to perform min-max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c59ddba1-c8b4-4cce-a87e-94d76865fa46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Feature1  Feature2\n",
      "0        10       100\n",
      "1        20       200\n",
      "2        30       300\n",
      "3        40       400\n",
      "4        50       500\n",
      "\n",
      "Scaled Data:\n",
      "   Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50],\n",
    "    'Feature2': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Convert data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using MinMaxScaler\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dff256a-dd66-48f9-861c-ddb18eb7a160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Ans 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffad12a-be08-424e-a7c0-0b6c5f39d194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Unit Vector technique, also known as normalization or vector normalization, scales each sample (row) in a\n",
    "# dataset to have a vector norm of 1. This technique is often used in machine learning when the magnitude of the individual\n",
    "# samples is important, but the direction does not matter.\n",
    "\n",
    "# In essence, unit vector scaling adjusts the values within each sample (row) without considering the distribution of values\n",
    "# across different features, unlike Min-Max scaling.\n",
    "\n",
    "# The formula for unit vector scaling is:\n",
    "    \n",
    "#     X_scaled = X/(∣∣X∣∣)\n",
    "\n",
    "# where:\n",
    "# X is the original vector.\n",
    "# ∣∣X∣∣ is the Euclidean norm (magnitude) of the vector.\n",
    "\n",
    "# In contrast, Min-Max scaling scales features to a specific range (like 0 to 1), considering the minimum and maximum values of\n",
    "# each feature independently.\n",
    "\n",
    "# Let's illustrate the difference with an example in Python, comparing unit vector scaling and Min-Max scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c595e0-ac9f-4f75-bc83-c2cf2b69705a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Feature1  Feature2\n",
      "0        10       100\n",
      "1        20       200\n",
      "2        30       300\n",
      "3        40       400\n",
      "4        50       500\n",
      "\n",
      "Min-Max Scaled Data:\n",
      "   Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n",
      "\n",
      "Unit Vector Scaled Data:\n",
      "   Feature1  Feature2\n",
      "0  0.099504  0.995037\n",
      "1  0.099504  0.995037\n",
      "2  0.099504  0.995037\n",
      "3  0.099504  0.995037\n",
      "4  0.099504  0.995037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50],\n",
    "    'Feature2': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Convert data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "minmax_scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Unit vector scaling\n",
    "normalizer = Normalizer(norm='l2')  # Using L2 normalization for unit vector scaling\n",
    "unit_vector_scaled_data = normalizer.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to DataFrames\n",
    "minmax_scaled_df = pd.DataFrame(minmax_scaled_data, columns=df.columns)\n",
    "unit_vector_scaled_df = pd.DataFrame(unit_vector_scaled_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nMin-Max Scaled Data:\")\n",
    "print(minmax_scaled_df)\n",
    "print(\"\\nUnit Vector Scaled Data:\")\n",
    "print(unit_vector_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47901419-a9be-420f-91b7-ada2647c395e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Ans 03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b317bb4c-00bf-4d20-9ffd-36ad5e3b1bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA, or Principal Component Analysis, is a technique used for dimensionality reduction in data by transforming\n",
    "# the original features into a new set of orthogonal (uncorrelated) features called principal components. It aims to\n",
    "# capture the maximum variance in the data while reducing the number of features.\n",
    "\n",
    "# The steps involved in PCA are:\n",
    "\n",
    "# Standardization: Standardize the data by subtracting the mean and scaling to unit variance.\n",
    "# Compute Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "# Compute Eigenvectors and Eigenvalues: Perform eigen decomposition on the covariance matrix to obtain eigenvectors and eigenvalues.\n",
    "# Select Principal Components: Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with\n",
    "# the highest eigenvalues (explained variance) are the principal components.\n",
    "# Project Data onto Principal Components: Transform the data onto the new feature space formed by the selected principal components.\n",
    "\n",
    "# Here's an example of PCA using Python's PCA module from sklearn.decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75e17265-ec9a-4208-9438-d37d648eea2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0        5.1        3.5        1.4        0.2\n",
      "1        4.9        3.0        1.4        0.2\n",
      "2        4.7        3.2        1.3        0.2\n",
      "3        4.6        3.1        1.5        0.2\n",
      "4        5.0        3.6        1.4        0.2\n",
      "\n",
      "PCA Transformed Data (First 5 rows):\n",
      "        PC1       PC2\n",
      "0 -2.684126  0.319397\n",
      "1 -2.714142 -0.177001\n",
      "2 -2.888991 -0.144949\n",
      "3 -2.745343 -0.318299\n",
      "4 -2.728717  0.326755\n",
      "\n",
      "Explained Variance Ratio:\n",
      "[0.92461872 0.05306648]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load Iris dataset as an example\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "columns = [f\"Feature_{i+1}\" for i in range(data.shape[1])]\n",
    "\n",
    "# Convert data to a DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components for illustration\n",
    "pca_result = pca.fit_transform(df)\n",
    "\n",
    "# Convert PCA results to a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "print(\"\\nPCA Transformed Data (First 5 rows):\")\n",
    "print(pca_df.head())\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d532b627-333b-43f6-a8e1-e9bb29c3bd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Ans 04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acef18cf-8f8f-4493-ac18-9e3d25ba5fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA and feature extraction are inherently related concepts. PCA can be utilized as a feature extraction technique\n",
    "# wherein it extracts a reduced set of features (principal components) from the original features, effectively performing\n",
    "# dimensionality reduction while retaining the most significant information.\n",
    "\n",
    "# Feature extraction involves deriving new features from the original set of features, aiming to represent the data more\n",
    "# effectively or efficiently. PCA achieves this by identifying the directions, or axes, in the data that capture the most\n",
    "# variance. These axes become the principal components, which are a linear combination of the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493a80b8-5052-4f1d-a29c-b71e5556a5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0        5.1        3.5        1.4        0.2\n",
      "1        4.9        3.0        1.4        0.2\n",
      "2        4.7        3.2        1.3        0.2\n",
      "3        4.6        3.1        1.5        0.2\n",
      "4        5.0        3.6        1.4        0.2\n",
      "\n",
      "PCA Extracted Features (First 5 rows):\n",
      "        PC1       PC2\n",
      "0 -2.684126  0.319397\n",
      "1 -2.714142 -0.177001\n",
      "2 -2.888991 -0.144949\n",
      "3 -2.745343 -0.318299\n",
      "4 -2.728717  0.326755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load Iris dataset as an example\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "columns = [f\"Feature_{i+1}\" for i in range(data.shape[1])]\n",
    "\n",
    "# Convert data to a DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components for illustration\n",
    "pca_result = pca.fit_transform(df)\n",
    "\n",
    "# Convert PCA results to a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "print(\"\\nPCA Extracted Features (First 5 rows):\")\n",
    "print(pca_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9d61e18-dbe4-4d7f-8d5f-9161d5085c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Ans 05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce312cd9-fa82-45d9-9002-27c9db60e53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In the context of building a recommendation system for a food delivery service, Min-Max scaling can be a valuable\n",
    "# preprocessing step to normalize the numerical features like price, rating, and delivery time. This normalization technique\n",
    "# would bring these features to a common scale, typically between 0 and 1, allowing them to contribute equally to the recommendation\n",
    "# model without affecting their relative differences.\n",
    "\n",
    "\n",
    "# Here's a step-by-step explanation of how Min-Max scaling can be applied to preprocess the data:\n",
    "\n",
    "# 1. Identify Numerical Features: Identify the numerical features in the dataset that require scaling. In your case, it's likely to be\n",
    "# features like price, rating, and delivery time.\n",
    "\n",
    "\n",
    "# 2. Use Min-Max Scaling: Apply Min-Max scaling to these numerical features. For each feature, follow these steps:\n",
    "\n",
    "# Compute the minimum and maximum values of the feature.\n",
    "\n",
    "# Scale the values of the feature using the Min-Max scaling formula:\n",
    "    \n",
    "#     X_scaled = (X - X_min)/(X_max − X_min)\n",
    "    \n",
    "# where:\n",
    "# X is the original value of the feature.\n",
    "# X_min is the minimum value of the feature.\n",
    "# X_max is the maximum value of the feature.\n",
    "\n",
    "# 3. Implement Min-Max Scaling in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d00a216f-7efd-4f2c-b1bc-15dca2ded2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['price', 'rating', 'delivery_time'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Scale the numerical features\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df[numerical_features] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df[numerical_features])\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['price', 'rating', 'delivery_time'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "# Assuming df is your DataFrame containing the dataset\n",
    "\n",
    "# Select numerical features for scaling\n",
    "numerical_features = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the numerical features\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "103d9b4c-46b9-470a-bd26-ecc57d9a1a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Check the Scaled Data: Verify that the scaling has been applied correctly and that the values for each\n",
    "# numerical feature now fall within the range of 0 to 1.\n",
    "\n",
    "# 5. Use Scaled Data in the Recommendation System: Utilize the preprocessed, scaled data as input to your recommendation\n",
    "# system model. The scaled features will contribute equally to the model without the bias caused by differing scales.\n",
    "\n",
    "# By performing Min-Max scaling, the recommendation system can effectively use these normalized numerical features to provide\n",
    "# accurate recommendations, ensuring that no single feature dominates the model due to its larger scale or range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "567f9328-4050-4bed-81ad-71c817b24d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Ans 06:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9553538d-2c54-4dd0-bd42-c9807bc04198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In the context of predicting stock prices using a dataset with numerous features like company financial data\n",
    "# and market trends, PCA (Principal Component Analysis) can be instrumental in reducing the dimensionality of the dataset\n",
    "# while retaining the most significant information.\n",
    "\n",
    "# Here's a step-by-step approach on how PCA can be utilized for dimensionality reduction in a stock price prediction project:\n",
    "\n",
    "# Data Preparation:\n",
    "# Identify the features relevant to stock price prediction in your dataset, which could include financial indicators, market trends,\n",
    "# stock volumes, etc.\n",
    "\n",
    "# Standardization:\n",
    "# Standardize the features to ensure they have a mean of 0 and a standard deviation of 1. This step is crucial for PCA as it works\n",
    "# best with standardized data.\n",
    "\n",
    "# Apply PCA:\n",
    "# Use PCA to identify the principal components that capture the most variance in the dataset.\n",
    "# Decide on the number of principal components to retain. This decision can be based on the cumulative explained variance ratio.\n",
    "\n",
    "# Implement PCA in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a2d50b-1fb1-4266-983e-d9c1da75ad6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['feature_1', 'feature_2', Ellipsis], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# Include relevant features\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Subset the dataframe with selected features\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m subset_data \u001b[38;5;241m=\u001b[39m df[selected_features]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['feature_1', 'feature_2', Ellipsis], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data (assuming df contains the dataset)\n",
    "# Select features for PCA\n",
    "selected_features = ['feature_1', 'feature_2', ...]  # Include relevant features\n",
    "\n",
    "# Subset the dataframe with selected features\n",
    "subset_data = df[selected_features]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(subset_data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)  # Choose the number of components\n",
    "pca_result = pca.fit_transform(standardized_data)\n",
    "\n",
    "# Create a DataFrame with PCA results\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[f'PC{i+1}' for i in range(pca_result.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5ab4ad-3b60-42a2-a03a-e14902e8e4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze Explained Variance:\n",
    "# Check the explained variance ratio (pca.explained_variance_ratio_) to understand how much variance each principal component captures.\n",
    "# Consider the cumulative explained variance to decide the number of components to retain.\n",
    "\n",
    "# Utilize Reduced Dimensionality Data:\n",
    "# Use the transformed data with reduced dimensions (principal components) in your stock price prediction model.\n",
    "\n",
    "# By applying PCA, you'll reduce the dimensionality of the dataset while retaining the most critical information captured by the\n",
    "# principal components. This can lead to more efficient models by focusing on the most informative features and potentially improving\n",
    "# prediction accuracy and reducing overfitting, especially when dealing with a large number of features in stock market datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95131079-969e-49c0-b3fd-a4f34bef0411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Ans 07:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4273184-f1dc-4b7e-a47f-984b3d077a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To perform Min-Max scaling on a dataset to transform values to a range of -1 to 1, you can use the following\n",
    "# formula:\n",
    "    \n",
    "#     X_scaled = ((X − X_min)/(X_max − X_min)) × (max_range − min_range) + min_range\n",
    "    \n",
    "# In this case, the original dataset is [1, 5, 10, 15, 20], and we want to transform these values to a range of -1 to 1. The steps\n",
    "# involved are:\n",
    "\n",
    "# Compute X_min and X_max from the original dataset.\n",
    "# Use the Min-Max scaling formula to transform the values to the desired range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bee5cd7c-a35f-4222-8399-8484e0b4f0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [1, 5, 10, 15, 20]\n",
      "Min-Max Scaled Data (-1 to 1): [-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 5, 10, 15, 20]\n",
    "min_range = -1\n",
    "max_range = 1\n",
    "\n",
    "# Compute X_min and X_max\n",
    "X_min = min(data)\n",
    "X_max = max(data)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = [\n",
    "    ((x - X_min) / (X_max - X_min)) * (max_range - min_range) + min_range\n",
    "    for x in data\n",
    "]\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Min-Max Scaled Data (-1 to 1):\", scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb4f981f-82b5-4653-bd0b-ced6c142dd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code snippet will compute the Min-Max scaling for the provided dataset [1, 5, 10, 15, 20], transforming the\n",
    "# values to fit within the range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "893288c0-12ba-4b8c-b1ac-66fcab70d62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#Ans 08:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f841e606-23b5-469b-8731-083080fcab4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deciding on the number of principal components to retain in PCA involves considering the trade-off between\n",
    "# reducing dimensionality and retaining sufficient variance in the data. Here are steps to determine the number of\n",
    "# principal components to retain for the given features: height, weight, age, gender, and blood pressure:\n",
    "\n",
    "# 1. Standardize the Data: Scale the features to have zero mean and unit variance as PCA works best on standardized data.\n",
    "\n",
    "# 2. Apply PCA:\n",
    "# Use PCA to compute the principal components from the standardized data.\n",
    "# Analyze the explained variance ratio (explained_variance_ratio_) for each principal component.\n",
    "\n",
    "# 3. Select Number of Components:\n",
    "# Examine the cumulative explained variance ratio to understand how much variance is retained as the number of components increases.\n",
    "# Retain enough principal components that capture a significant portion of the total variance in the data. A common rule is to retain\n",
    "# components that explain a high percentage of the total variance (e.g., 95% or more).\n",
    "# Alternatively, consider the \"elbow\" in the explained variance plot, where adding more components yields diminishing returns in\n",
    "# explained variance.\n",
    "\n",
    "# Let's assume we have standardized data and apply PCA to determine the number of principal components to retain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "727c248f-1101-460e-b1cf-3d4216b73229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 1.  5. 10. 15. 20.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sample data (replace this with your actual dataset)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming 'data' contains the features: height, weight, age, gender, blood pressure\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 9\u001b[0m standardized_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(data)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Apply PCA\u001b[39;00m\n\u001b[0;32m     12\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA()\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 873\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    874\u001b[0m     X,\n\u001b[0;32m    875\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    876\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    877\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    878\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mD:\\Users\\MY PC\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    945\u001b[0m         )\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 1.  5. 10. 15. 20.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data (replace this with your actual dataset)\n",
    "# Assuming 'data' contains the features: height, weight, age, gender, blood pressure\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Determine number of components to retain\n",
    "cumulative_variance = explained_variance_ratio.cumsum()\n",
    "num_components = len([variance for variance in cumulative_variance if variance <= 0.95])  # Retain 95% variance\n",
    "\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Variance Ratio:\", cumulative_variance)\n",
    "print(\"Number of Components to Retain for 95% variance:\", num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9eeffc5a-cfe0-4fa9-8b72-0ffaa9940002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code snippet performs PCA on the standardized data and calculates the explained variance ratio for each\n",
    "# principal component. The num_components variable is computed based on retaining 95% of the variance. You can adjust\n",
    "# this threshold based on the trade-off between dimensionality reduction and retaining information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa8cc864-bfa9-460b-ac1d-5124e4597d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
