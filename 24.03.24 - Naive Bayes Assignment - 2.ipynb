{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b54aca5-23a2-43a0-8ab9-29a6f2e99a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4799b1-d4e9-4dce-85b8-03d13e0b2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S be the event that an employee is a smoker.\n",
    "# H be the event that an employee uses the health insurance plan.\n",
    "    \n",
    "# We are given:\n",
    "# P(H), the probability that an employee uses the health insurance plan, which is 0.70.\n",
    "# P(S∣H), the conditional probability that an employee is a smoker given that he/she uses the health insurance plan, which is 0.40.\n",
    "\n",
    "# We want to find P(S∣H), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "# Using Bayes' theorem:\n",
    "\n",
    "#     P(S∣H) = (P(H∣S)×P(S))/P(H)\n",
    "\n",
    "# Where:\n",
    "# P(S∣H) is the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "# P(H∣S) is the probability that an employee uses the health insurance plan given that he/she is a smoker.\n",
    "# P(S) is the probability that an employee is a smoker, which is not given explicitly but can be calculated using the information provided.\n",
    "# P(H) is the probability that an employee uses the health insurance plan.\n",
    "\n",
    "# We can calculate P(S) using the total probability rule:\n",
    "\n",
    "#     P(S) = P(S∣H)×P(H)+P(S∣¬H)×P(¬H)\n",
    "\n",
    "# Where:\n",
    "# P(S∣H) is the probability that an employee is a smoker given that he/she uses the health insurance plan (given as 0.40).\n",
    "# P(¬H) is the probability that an employee does not use the health insurance plan, which is 1 − 0.70 = 0.30\n",
    "# P(S∣¬H) is the probability that an employee is a smoker given that he/she does not use the health insurance plan, which is not given explicitly.\n",
    "\n",
    "# Let's calculate P(S) and then use Bayes' theorem to find P(S∣H):\n",
    "\n",
    "#     P(S) = 0.40×0.70 + P(S∣¬H)×0.30\n",
    "#     P(S) = 0.28+0.30×P(S∣¬H)\n",
    "\n",
    "# Now, we need the probability that an employee is a smoker given that he/she does not use the health insurance plan, P(S∣¬H). We can find this by using the fact\n",
    "# that the set of employees who do not use the health insurance plan consists of both smokers and non-smokers. Therefore, P(S∣¬H) can be calculated as:\n",
    "\n",
    "#     P(S∣¬H) = 1−P(¬S∣¬H)\n",
    "\n",
    "# Where \n",
    "# P(¬S∣¬H) is the probability that an employee is not a smoker given that he/she does not use the health insurance plan. This probability is the complement of the\n",
    "# probability that an employee is a smoker given that he/she does not use the health insurance plan. Since the proportion of smokers in the population is not given,\n",
    "# we'll assume it's the same as the proportion of smokers among those who use the health insurance plan, which is 0.40.\n",
    "\n",
    "#     P(S∣¬H) = 1−P(¬S∣¬H) = 1−(1−P(S∣¬H)) = P(S∣¬H)\n",
    "\n",
    "# Now, let's plug this back into the equation for P(S) and solve for P(S∣¬H):\n",
    "\n",
    "#     P(S) = 0.28 + 0.30 × P(S∣¬H)\n",
    "#     0.40 = 0.28 + 0.30 × P(S∣¬H)\n",
    "#     0.12 = 0.30 × P(S∣¬H)\n",
    "#     P(S∣¬H) = 0.12/0.30 = 0.40\n",
    "\n",
    "# Now that we have P(S∣¬H), we can use Bayes' theorem to find P(S∣H):\n",
    "\n",
    "#     P(S∣H) = (P(H∣S) × P(S)) / P(H)\n",
    "#     P(S∣H) = (0.40 × 0.40) / 0.70\n",
    "#     P(S∣H) ≈ 0.2286\n",
    "\n",
    "# So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.2286."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100c9c75-3681-4647-a39c-f722ce676380",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "# Ans 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7cd2aa0-8484-46df-a263-e95484e9ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the assumptions they make about the distribution of\n",
    "# features and how they handle feature occurrences.\n",
    "\n",
    "# 1. Bernoulli Naive Bayes:\n",
    "# a. Assumes that features are binary-valued (i.e., they occur or do not occur).\n",
    "# b. Typically used for document classification tasks where each feature represents the presence or absence of a term in a document (e.g., bag-of-words\n",
    "# representation).\n",
    "# c. It models the presence or absence of each feature independently for each class.\n",
    "# d. Suited for tasks where only the presence of features matters and the order or frequency of occurrences is not considered.\n",
    "\n",
    "# 2. Multinomial Naive Bayes:\n",
    "# a. Assumes that features represent counts or frequencies of events.\n",
    "# b. Commonly used for text classification tasks where features represent word counts or TF-IDF values.\n",
    "# c. It models the frequency of occurrence of each feature independently for each class.\n",
    "# d. Suited for tasks where the frequency or occurrence counts of features are important, such as in text classification or document analysis.\n",
    "\n",
    "    \n",
    "# In summary, Bernoulli Naive Bayes is suitable for binary-valued features, while Multinomial Naive Bayes is suitable for features representing counts or\n",
    "# frequencies. The choice between the two depends on the nature of the features in the dataset and the assumptions that are reasonable for the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67fb079-1e64-48f5-93d1-3298d556b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "# Ans 03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70655db1-829e-4186-824d-bbf07de16f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Bernoulli Naive Bayes, missing values are typically handled by ignoring them during model training and classification. Since Bernoulli Naive\n",
    "# Bayes assumes that features are binary-valued (i.e., they occur or do not occur), missing values can be treated as if the feature is not present for\n",
    "# the instance.\n",
    "\n",
    "# During model training, the algorithm calculates the probabilities of feature occurrence (or absence) for each class based on the observed data. If a feature\n",
    "# is missing for a particular instance, it is simply not considered when computing these probabilities.\n",
    "\n",
    "# Similarly, during classification, if a feature value is missing for an instance, it does not contribute to the likelihood computation for that class. Instead,\n",
    "# the algorithm calculates the likelihood based on the observed features for the instance.\n",
    "\n",
    "# In practical terms, missing values in Bernoulli Naive Bayes are handled implicitly by the algorithm and do not require any special treatment. However, it's\n",
    "# important to preprocess the data to ensure that missing values are properly handled, such as by imputing them with a default value or using techniques like mean\n",
    "# imputation or most frequent imputation, if appropriate for the dataset and problem domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15daad82-60e9-4413-9d96-6df6997fd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "# Ans 04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5144e35-3367-4262-be12-9bdc91756a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, Gaussian Naive Bayes can be used for multi-class classification tasks.\n",
    "\n",
    "# In Gaussian Naive Bayes, it's assumed that the features follow a Gaussian (normal) distribution within each class. This assumption holds for continuous-valued\n",
    "# features, where the probability density function of each feature is estimated using the mean and variance of the feature values within each class.\n",
    "\n",
    "# For multi-class classification, Gaussian Naive Bayes extends naturally by estimating the parameters (mean and variance) for each feature within each class. When\n",
    "# making predictions for a new instance, the algorithm calculates the likelihood of the observed feature values under each class's Gaussian distribution and\n",
    "# combines this with the prior probabilities of the classes to compute the posterior probability of each class. The class with the highest posterior probability\n",
    "# is then predicted for the instance.\n",
    "\n",
    "# In summary, Gaussian Naive Bayes can indeed be used for multi-class classification by extending the Gaussian distribution assumption to each class and estimating\n",
    "# the parameters separately for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96dd694d-8840-4f71-a8b7-0dfa84949b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "# Ans 05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240f5b18-f732-4bee-8e6e-7500700f8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620134e3-9608-4525-a2d9-04f96f7fe917",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spambase.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37592ea3-60d4-48c6-83d5-e2493f45b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    'word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
    "    'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses', 'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
    "    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "    'word_freq_telnet', 'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n",
    "    'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(',\n",
    "    'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total', 'Class'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b66ea1e-b557-4228-8ff3-764f41c324db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1aebad8-8ecc-49b1-a2fc-78e429bcd9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0               0.00               0.64           0.64           0.0   \n",
       "1               0.21               0.28           0.50           0.0   \n",
       "2               0.06               0.00           0.71           0.0   \n",
       "3               0.00               0.00           0.00           0.0   \n",
       "4               0.00               0.00           0.00           0.0   \n",
       "...              ...                ...            ...           ...   \n",
       "4596            0.31               0.00           0.62           0.0   \n",
       "4597            0.00               0.00           0.00           0.0   \n",
       "4598            0.30               0.00           0.30           0.0   \n",
       "4599            0.96               0.00           0.00           0.0   \n",
       "4600            0.00               0.00           0.65           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0              0.32            0.00              0.00                0.00   \n",
       "1              0.14            0.28              0.21                0.07   \n",
       "2              1.23            0.19              0.19                0.12   \n",
       "3              0.63            0.00              0.31                0.63   \n",
       "4              0.63            0.00              0.31                0.63   \n",
       "...             ...             ...               ...                 ...   \n",
       "4596           0.00            0.31              0.00                0.00   \n",
       "4597           0.00            0.00              0.00                0.00   \n",
       "4598           0.00            0.00              0.00                0.00   \n",
       "4599           0.32            0.00              0.00                0.00   \n",
       "4600           0.00            0.00              0.00                0.00   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0                0.00            0.00  ...        0.000        0.000   \n",
       "1                0.00            0.94  ...        0.000        0.132   \n",
       "2                0.64            0.25  ...        0.010        0.143   \n",
       "3                0.31            0.63  ...        0.000        0.137   \n",
       "4                0.31            0.63  ...        0.000        0.135   \n",
       "...               ...             ...  ...          ...          ...   \n",
       "4596             0.00            0.00  ...        0.000        0.232   \n",
       "4597             0.00            0.00  ...        0.000        0.000   \n",
       "4598             0.00            0.00  ...        0.102        0.718   \n",
       "4599             0.00            0.00  ...        0.000        0.057   \n",
       "4600             0.00            0.00  ...        0.000        0.000   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0             0.0        0.778        0.000        0.000   \n",
       "1             0.0        0.372        0.180        0.048   \n",
       "2             0.0        0.276        0.184        0.010   \n",
       "3             0.0        0.137        0.000        0.000   \n",
       "4             0.0        0.135        0.000        0.000   \n",
       "...           ...          ...          ...          ...   \n",
       "4596          0.0        0.000        0.000        0.000   \n",
       "4597          0.0        0.353        0.000        0.000   \n",
       "4598          0.0        0.000        0.000        0.000   \n",
       "4599          0.0        0.000        0.000        0.000   \n",
       "4600          0.0        0.125        0.000        0.000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                          3.756                          61   \n",
       "1                          5.114                         101   \n",
       "2                          9.821                         485   \n",
       "3                          3.537                          40   \n",
       "4                          3.537                          40   \n",
       "...                          ...                         ...   \n",
       "4596                       1.142                           3   \n",
       "4597                       1.555                           4   \n",
       "4598                       1.404                           6   \n",
       "4599                       1.147                           5   \n",
       "4600                       1.250                           5   \n",
       "\n",
       "      capital_run_length_total  Class  \n",
       "0                          278      1  \n",
       "1                         1028      1  \n",
       "2                         2259      1  \n",
       "3                          191      1  \n",
       "4                          191      1  \n",
       "...                        ...    ...  \n",
       "4596                        88      0  \n",
       "4597                        14      0  \n",
       "4598                       118      0  \n",
       "4599                        78      0  \n",
       "4600                        40      0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3c207e-752f-4671-ab6a-7069518aa4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    2788\n",
       "1    1813\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b99ad4a5-b270-4454-8e80-f9896c2c9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fab7c05-e9f8-4b95-8167-128a05992f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the dataset\n",
    "oversample = SMOTE()\n",
    "X,y = oversample.fit_resample(data[['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
    "    'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses', 'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
    "    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "    'word_freq_telnet', 'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n",
    "    'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(',\n",
    "    'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total']], data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e4a5db-bb37-4333-aedc-0ea496477910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(X, columns = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
    "    'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses', 'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
    "    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "    'word_freq_telnet', 'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n",
    "    'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(',\n",
    "    'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total'])\n",
    "df2 = pd.DataFrame(y, columns = ['Class'])\n",
    "oversample_df = pd.concat([df1, df2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a7be68d-19c3-475f-b704-fd01d7423374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.756000</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>5.114000</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>9.821000</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.537000</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.537000</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.707108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.277655</td>\n",
       "      <td>13</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.091623</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.152450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.813000</td>\n",
       "      <td>494</td>\n",
       "      <td>1458</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5574</th>\n",
       "      <td>0.131508</td>\n",
       "      <td>0.131508</td>\n",
       "      <td>0.227358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366602</td>\n",
       "      <td>0.061886</td>\n",
       "      <td>0.061886</td>\n",
       "      <td>0.061886</td>\n",
       "      <td>0.533769</td>\n",
       "      <td>1.864324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.069038</td>\n",
       "      <td>0.124267</td>\n",
       "      <td>4.687089</td>\n",
       "      <td>71</td>\n",
       "      <td>1214</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>0.484541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481845</td>\n",
       "      <td>0.238964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.090522</td>\n",
       "      <td>0.212890</td>\n",
       "      <td>0.271651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452596</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.248658</td>\n",
       "      <td>13</td>\n",
       "      <td>187</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5576 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0           0.000000           0.640000       0.640000           0.0   \n",
       "1           0.210000           0.280000       0.500000           0.0   \n",
       "2           0.060000           0.000000       0.710000           0.0   \n",
       "3           0.000000           0.000000       0.000000           0.0   \n",
       "4           0.000000           0.000000       0.000000           0.0   \n",
       "...              ...                ...            ...           ...   \n",
       "5571        0.000000           0.000000       0.000000           0.0   \n",
       "5572        0.000000           0.000000       0.000000           0.0   \n",
       "5573        0.090000           0.000000       0.090000           0.0   \n",
       "5574        0.131508           0.131508       0.227358           0.0   \n",
       "5575        0.484541           0.000000       0.756192           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0          0.320000        0.000000          0.000000            0.000000   \n",
       "1          0.140000        0.280000          0.210000            0.070000   \n",
       "2          1.230000        0.190000          0.190000            0.120000   \n",
       "3          0.630000        0.000000          0.310000            0.630000   \n",
       "4          0.630000        0.000000          0.310000            0.630000   \n",
       "...             ...             ...               ...                 ...   \n",
       "5571       0.000000        0.000000          0.000000            0.000000   \n",
       "5572       0.000000        0.000000          0.000000            0.000000   \n",
       "5573       0.390000        0.090000          0.090000            0.000000   \n",
       "5574       0.366602        0.061886          0.061886            0.061886   \n",
       "5575       0.481845        0.238964          0.000000            1.090522   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0            0.000000        0.000000  ...     0.000000     0.000000   \n",
       "1            0.000000        0.940000  ...     0.000000     0.132000   \n",
       "2            0.640000        0.250000  ...     0.010000     0.143000   \n",
       "3            0.310000        0.630000  ...     0.000000     0.137000   \n",
       "4            0.310000        0.630000  ...     0.000000     0.135000   \n",
       "...               ...             ...  ...          ...          ...   \n",
       "5571         0.000000        3.707108  ...     0.260226     0.000000   \n",
       "5572         0.000000        0.000000  ...     0.000000     0.000000   \n",
       "5573         0.190000        0.290000  ...     0.000000     0.137450   \n",
       "5574         0.533769        1.864324  ...     0.000000     0.094320   \n",
       "5575         0.212890        0.271651  ...     0.000000     0.120701   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0             0.0     0.778000     0.000000     0.000000   \n",
       "1             0.0     0.372000     0.180000     0.048000   \n",
       "2             0.0     0.276000     0.184000     0.010000   \n",
       "3             0.0     0.137000     0.000000     0.000000   \n",
       "4             0.0     0.135000     0.000000     0.000000   \n",
       "...           ...          ...          ...          ...   \n",
       "5571          0.0     0.238046     0.000000     0.000000   \n",
       "5572          0.0     0.000000     0.000000     0.000000   \n",
       "5573          0.0     0.320900     0.152450     0.000000   \n",
       "5574          0.0     0.296000     0.069038     0.124267   \n",
       "5575          0.0     0.452596     0.009794     0.000000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756000                          61   \n",
       "1                       5.114000                         101   \n",
       "2                       9.821000                         485   \n",
       "3                       3.537000                          40   \n",
       "4                       3.537000                          40   \n",
       "...                          ...                         ...   \n",
       "5571                    4.277655                          13   \n",
       "5572                    1.091623                           1   \n",
       "5573                    6.813000                         494   \n",
       "5574                    4.687089                          71   \n",
       "5575                    1.248658                          13   \n",
       "\n",
       "      capital_run_length_total  Class  \n",
       "0                          278      1  \n",
       "1                         1028      1  \n",
       "2                         2259      1  \n",
       "3                          191      1  \n",
       "4                          191      1  \n",
       "...                        ...    ...  \n",
       "5571                        78      1  \n",
       "5572                        10      1  \n",
       "5573                      1458      1  \n",
       "5574                      1214      1  \n",
       "5575                       187      1  \n",
       "\n",
       "[5576 rows x 58 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28dec8ef-c68b-433a-a0dd-d550ed14a037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "1    2788\n",
       "0    2788\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79153011-86b9-4ab3-953d-b85482ce504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = oversample_df.drop('Class', axis=1)\n",
    "y = oversample_df.loc[:,'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3717a2dd-ccdd-4d85-9891-372e7df85311",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e81a994-5823-4433-be47-4dbb0952e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d71b77d9-678c-41cb-aab6-c48d2cc8acfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.88\n",
      "Standard Deviation of Accuracy: 0.02\n",
      "Mean Precision: 0.92\n",
      "Standard Deviation of Precision: 0.02\n",
      "Mean Recall: 0.83\n",
      "Standard Deviation of Recall: 0.02\n",
      "Mean F1: 0.87\n",
      "Standard Deviation of F1: 0.02\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = bnb.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    cv_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "mean_accuracy = sum(cv_scores) / len(cv_scores)\n",
    "std_dev_accuracy = np.std(cv_scores)\n",
    "\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "std_dev_precision = np.std(precision_scores)\n",
    "\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "std_dev_recall = np.std(recall_scores)\n",
    "\n",
    "mean_f1 = sum(f1_scores) / len(f1_scores)\n",
    "std_dev_f1 = np.std(f1_scores)\n",
    "\n",
    "print(\"Mean Accuracy:\", round(mean_accuracy, 2))\n",
    "print(\"Standard Deviation of Accuracy:\", round(std_dev_accuracy, 2))\n",
    "\n",
    "print(\"Mean Precision:\", round(mean_precision, 2))\n",
    "print(\"Standard Deviation of Precision:\", round(std_dev_precision, 2))\n",
    "\n",
    "print(\"Mean Recall:\", round(mean_recall, 2))\n",
    "print(\"Standard Deviation of Recall:\", round(std_dev_recall, 2))\n",
    "\n",
    "print(\"Mean F1:\", round(mean_f1, 2))\n",
    "print(\"Standard Deviation of F1:\", round(std_dev_f1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4987a83f-b940-4baa-a93e-efc5634fc56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.78\n",
      "Standard Deviation of Accuracy: 0.02\n",
      "Mean Precision: 0.81\n",
      "Standard Deviation of Precision: 0.01\n",
      "Mean Recall: 0.73\n",
      "Standard Deviation of Recall: 0.03\n",
      "Mean F1: 0.77\n",
      "Standard Deviation of F1: 0.02\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = mnb.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    cv_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "mean_accuracy = sum(cv_scores) / len(cv_scores)\n",
    "std_dev_accuracy = np.std(cv_scores)\n",
    "\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "std_dev_precision = np.std(precision_scores)\n",
    "\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "std_dev_recall = np.std(recall_scores)\n",
    "\n",
    "mean_f1 = sum(f1_scores) / len(f1_scores)\n",
    "std_dev_f1 = np.std(f1_scores)\n",
    "\n",
    "print(\"Mean Accuracy:\", round(mean_accuracy, 2))\n",
    "print(\"Standard Deviation of Accuracy:\", round(std_dev_accuracy, 2))\n",
    "\n",
    "print(\"Mean Precision:\", round(mean_precision, 2))\n",
    "print(\"Standard Deviation of Precision:\", round(std_dev_precision, 2))\n",
    "\n",
    "print(\"Mean Recall:\", round(mean_recall, 2))\n",
    "print(\"Standard Deviation of Recall:\", round(std_dev_recall, 2))\n",
    "\n",
    "print(\"Mean F1:\", round(mean_f1, 2))\n",
    "print(\"Standard Deviation of F1:\", round(std_dev_f1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f6ebae8-e54d-436f-9de7-81c4dee7f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.85\n",
      "Standard Deviation of Accuracy: 0.01\n",
      "Mean Precision: 0.79\n",
      "Standard Deviation of Precision: 0.02\n",
      "Mean Recall: 0.96\n",
      "Standard Deviation of Recall: 0.02\n",
      "Mean F1: 0.87\n",
      "Standard Deviation of F1: 0.01\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    gnb.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gnb.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    cv_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "mean_accuracy = sum(cv_scores) / len(cv_scores)\n",
    "std_dev_accuracy = np.std(cv_scores)\n",
    "\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "std_dev_precision = np.std(precision_scores)\n",
    "\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "std_dev_recall = np.std(recall_scores)\n",
    "\n",
    "mean_f1 = sum(f1_scores) / len(f1_scores)\n",
    "std_dev_f1 = np.std(f1_scores)\n",
    "\n",
    "print(\"Mean Accuracy:\", round(mean_accuracy, 2))\n",
    "print(\"Standard Deviation of Accuracy:\", round(std_dev_accuracy, 2))\n",
    "\n",
    "print(\"Mean Precision:\", round(mean_precision, 2))\n",
    "print(\"Standard Deviation of Precision:\", round(std_dev_precision, 2))\n",
    "\n",
    "print(\"Mean Recall:\", round(mean_recall, 2))\n",
    "print(\"Standard Deviation of Recall:\", round(std_dev_recall, 2))\n",
    "\n",
    "print(\"Mean F1:\", round(mean_f1, 2))\n",
    "print(\"Standard Deviation of F1:\", round(std_dev_f1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5a09c73-578a-4057-b05c-f03f7eed061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
