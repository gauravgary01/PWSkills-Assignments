{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a875b962-d1f3-4ca9-bd66-be777fbf184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cadf0f5-07b9-4c19-848b-c621a91981ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial functions and kernel functions are closely related in machine learning algorithms, especially in the context of kernel\n",
    "# methods such as Support Vector Machines (SVMs) and kernelized versions of algorithms like the Kernel Ridge Regression (KRR) and the Kernel PCA.\n",
    "\n",
    "# Polynomial Functions:\n",
    "# A polynomial function is a mathematical function that can be represented as a sum of powers of a variable, multiplied by coefficients.\n",
    "# In machine learning, polynomial functions are commonly used as basis functions for feature transformation. For example, in polynomial regression,\n",
    "# the input features are transformed using polynomial functions to capture nonlinear relationships between the features and the target variable.\n",
    "                                                                              \n",
    "# Kernel Functions:\n",
    "# A kernel function is a function that computes the similarity (or inner product) between pairs of data points in a high-dimensional feature space,\n",
    "# without explicitly transforming the data into that space.\n",
    "# Kernel functions are used in kernel methods to implicitly map the input data into a higher-dimensional space, where linear separation or other\n",
    "# operations become easier.\n",
    "# Common kernel functions include the linear kernel, polynomial kernel, Gaussian (RBF) kernel, sigmoid kernel, etc.\n",
    "\n",
    "# Relationship:\n",
    "# Polynomial functions can be used as basis functions in feature transformation, but they can also be used as kernel functions.\n",
    "# When a polynomial function is used as a kernel function, it computes the inner product between pairs of data points after applying the polynomial\n",
    "# transformation implicitly.\n",
    "# This means that instead of explicitly transforming the data into a higher-dimensional space using polynomial features, we can directly compute the\n",
    "# inner products between pairs of data points using the polynomial kernel function, thereby avoiding the computational cost and potential storage issues\n",
    "# associated with working in high-dimensional spaces.\n",
    "\n",
    "# In summary, polynomial functions can be used both as basis functions for feature transformation and as kernel functions for implicit feature mapping\n",
    "# in kernel methods. The choice of using polynomial functions as basis functions or kernel functions depends on the specific machine learning algorithm\n",
    "# and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7595238-ccd6-46be-bdae-4361352fef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# Ans 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "182ab65b-54ec-4bc9-9b46-9d2c32f28b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can implement an SVM with a polynomial kernel in Python using scikit-learn's SVC (Support Vector Classifier) class. Here's how\n",
    "# we can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e3d7ac-2be9-47bc-9e7f-aa358045a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM with polynomial kernel: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)  # Use degree=3 for a cubic polynomial kernel (you can adjust the degree as needed)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the SVM with polynomial kernel:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f38875-8749-48be-ab9c-ce6ed5c6e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this code:\n",
    "# 1. We first load the Iris dataset.\n",
    "# 2. Then, we split the dataset into training and testing sets.\n",
    "# 3. We initialize the SVM classifier with the polynomial kernel by specifying kernel='poly' and optionally providing the degree of the polynomial\n",
    "# using the degree parameter (default is 3).\n",
    "# 4. After that, we train the SVM classifier on the training data.\n",
    "# 5. Next, we predict labels for the testing set using the trained classifier.\n",
    "# 6. Finally, we compute the accuracy of the model by comparing the predicted labels with the actual labels of the testing set.\n",
    "\n",
    "# We can adjust the degree of the polynomial kernel by modifying the degree parameter to fit your specific problem. Additionally, you can experiment\n",
    "# with other parameters like C (regularization parameter) and gamma (kernel coefficient) to fine-tune the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b8967b-2208-414c-bb8e-cf7aca386f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# Ans 03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4dd6fb-2a85-4370-a9ef-ebea467152d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Support Vector Regression (SVR), the parameter epsilon (ε) determines the width of the ε-insensitive tube around the regression\n",
    "# line within which no penalty is associated with errors. Increasing the value of epsilon generally leads to a wider ε-insensitive tube, which\n",
    "# can have an impact on the number of support vectors.\n",
    "\n",
    "# Here's how increasing the value of epsilon affects the number of support vectors in SVR:\n",
    "\n",
    "# 1. Smaller Epsilon:\n",
    "# a. When epsilon is small, the ε-insensitive tube is narrow, and the SVR model is more sensitive to errors. This means that data points need to be\n",
    "# closer to the regression line to be considered as support vectors.\n",
    "# b. With a small epsilon, the SVR model is likely to have a larger number of support vectors because it requires more data points to be close to the\n",
    "# regression line to meet the ε-insensitive criterion.\n",
    "\n",
    "# 2. Larger Epsilon:\n",
    "# a. When epsilon is large, the ε-insensitive tube is wider, and the SVR model is less sensitive to errors. This means that data points can be farther\n",
    "# away from the regression line and still be considered as support vectors.\n",
    "# b. With a large epsilon, the SVR model is likely to have a smaller number of support vectors because it allows for more data points to be within the\n",
    "# wider ε-insensitive tube without penalty.\n",
    "\n",
    "# In summary, increasing the value of epsilon generally leads to a wider ε-insensitive tube, which can result in fewer support vectors in the SVR model.\n",
    "# Conversely, decreasing the value of epsilon tends to lead to a narrower ε-insensitive tube and potentially more support vectors. The choice of epsilon\n",
    "# depends on the specific problem and the desired trade-off between model simplicity (fewer support vectors) and model flexibility (wider ε-insensitive\n",
    "# tube)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a00d5d-0969-4038-9758-c96f2f5513eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# Ans 04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd206f3-71c7-4a9e-b431-7421c550445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regression (SVR) is a powerful regression algorithm that relies on several key parameters to control its performance.\n",
    "# Let's discuss how each parameter works and how it affects the performance of SVR:\n",
    "\n",
    "# 1. Kernel Function:\n",
    "# The kernel function determines the type of transformation applied to the input features. Common choices include linear, polynomial, radial basis\n",
    "# function (RBF), and sigmoid kernels.\n",
    "# The choice of kernel function affects the complexity and flexibility of the SVR model. Different kernel functions capture different types of\n",
    "# relationships between input features and target variables.\n",
    "# Example: If the relationship between input features and the target variable is highly nonlinear, using an RBF kernel might be more appropriate.\n",
    "# If the relationship is more linear, a linear kernel might suffice.\n",
    "\n",
    "# 2. C Parameter:\n",
    "# The C parameter controls the trade-off between maximizing the margin and minimizing the error. It balances the importance of having a simple decision\n",
    "# boundary (large margin) and accurately fitting the training data (small errors).\n",
    "# A smaller value of C allows for a wider margin and tolerates more errors in the training data. Conversely, a larger value of C penalizes errors more\n",
    "# heavily and results in a smaller margin.\n",
    "# Example: If the training data contains outliers or noise, it might be beneficial to use a smaller value of C to prevent overfitting to these points. On\n",
    "# the other hand, if the training data is clean and well-behaved, a larger value of C can lead to better generalization.\n",
    "\n",
    "# 3. Epsilon Parameter:\n",
    "# The epsilon parameter (ε) determines the width of the ε-insensitive tube around the regression line. Data points within this tube do not contribute to the\n",
    "# loss function, allowing the model to focus on fitting points outside the tube.\n",
    "# A smaller value of epsilon results in a narrower ε-insensitive tube, making the model more sensitive to errors. Conversely, a larger value of epsilon widens\n",
    "# the tube and makes the model less sensitive to errors.\n",
    "# Example: If the target variable has inherent noise or variability, using a larger epsilon can help the model generalize better by ignoring small fluctuations\n",
    "# in the data.\n",
    "\n",
    "# 4. Gamma Parameter:\n",
    "# The gamma parameter (γ) is specific to kernel functions like RBF and controls the influence of individual training samples on the decision boundary.\n",
    "# A smaller value of gamma results in a smoother decision boundary, with each training point having a more global influence. Conversely, a larger value of gamma\n",
    "# leads to a more complex decision boundary, with each point having a more local influence.\n",
    "# Example: If the dataset is large and contains many outliers, using a smaller value of gamma can help prevent overfitting by smoothing the decision boundary.\n",
    "# However, if the dataset is small or highly nonlinear, a larger value of gamma might be necessary to capture intricate patterns in the data.\n",
    "\n",
    "# In summary, each parameter in SVR plays a crucial role in determining the model's performance and generalization ability. Understanding how these parameters work\n",
    "# and how they interact with the data is essential for effectively tuning SVR models for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96089a5-b419-4b0e-bebc-937898ec644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# Ans 05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92b93be-4239-4030-97f4-8d07006de840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Best Parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate performance using classification report for more detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf', 'linear']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_classifier = SVC(**best_params)\n",
    "best_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(best_classifier, 'svm_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503358ac-494d-4b4f-812e-8fbd1d1e1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this implementation:\n",
    "\n",
    "# 1. We load the Iris dataset.\n",
    "# 2. Split the dataset into training and testing sets.\n",
    "# 3. Preprocess the data by scaling it using StandardScaler.\n",
    "# 4. Create an instance of the SVC classifier and train it on the training data.\n",
    "# 5. Predict the labels of the testing data using the trained classifier.\n",
    "# 6. Evaluate the performance of the classifier using accuracy and the classification report.\n",
    "# 7. Tune the hyperparameters of the SVC classifier using GridSearchCV.\n",
    "# 8. Train the tuned classifier on the entire dataset using the best parameters.\n",
    "# 9. Save the trained classifier to a file named 'svm_classifier.pkl' for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fae0676-db45-4e35-80e0-ca348cff6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
