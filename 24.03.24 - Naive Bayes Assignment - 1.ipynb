{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8741cd5-832f-4a25-99bb-ead000dc5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a7317b-b3b2-4feb-9e5d-4faa90bacafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' theorem is a fundamental concept in probability theory and statistics named after the Reverend Thomas Bayes. It provides\n",
    "# a way to update the probability of a hypothesis (an event or proposition) based on new evidence or information.\n",
    "\n",
    "# The theorem is expressed mathematically as:\n",
    "\n",
    "#     P(A∣B) = (P(B∣A)×P(A))/P(B)\n",
    "# ​\n",
    "# Where:\n",
    "# P(A∣B) is the posterior probability of event A given evidence B.\n",
    "# P(B∣A) is the likelihood, the probability of observing the evidence B given that A is true.\n",
    "# P(A) is the prior probability of A being true before considering any evidence.\n",
    "# P(B) is the marginal likelihood of observing the evidence B.\n",
    "\n",
    "# Bayes' theorem is widely used in various fields, including machine learning, statistics, and medical diagnosis, to make predictions and\n",
    "# update beliefs based on new information. It provides a principled way to reason under uncertainty and is foundational to Bayesian\n",
    "# statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbad7cd-6b39-4c80-8e9c-bdea1a05b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Ans 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0fa3ebe-1909-4185-912f-77217339dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already answered in answer 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d456db91-a936-4c06-bbef-43cddcc25e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Ans 03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9d7c9c-e3a7-4f83-9c96-fed52b3e9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' theorem is used in practice in various fields and applications, including but not limited to:\n",
    "\n",
    "# 1. Medical Diagnosis: It's used to calculate the probability of a patient having a particular disease given certain symptoms and medical\n",
    "# test results.\n",
    "# 2. Spam Filtering: In email systems, Bayes' theorem is used to classify emails as spam or non-spam based on the presence of certain words or\n",
    "# features in the email content.\n",
    "# 3. Machine Learning: In Bayesian inference and Bayesian networks, Bayes' theorem is used to update beliefs about model parameters or make\n",
    "# predictions based on observed data.\n",
    "# 4. Search Engines: Bayes' theorem is used in search engines to rank the relevance of web pages to a given query by considering factors such as\n",
    "# the frequency of keywords on the page and the popularity of the page.\n",
    "# 5. Financial Modeling: It's used in finance for risk assessment, portfolio management, and prediction of asset prices.\n",
    "# 6. Weather Forecasting: Bayes' theorem can be used in weather forecasting models to update the likelihood of different weather conditions based\n",
    "# on observed atmospheric data.\n",
    "# 7. Fault Diagnosis: In engineering and maintenance, Bayes' theorem is used to diagnose faults in systems based on observed symptoms and historical\n",
    "# data.\n",
    "\n",
    "# In all these applications, Bayes' theorem provides a systematic framework for updating beliefs or probabilities based on new evidence, leading\n",
    "# to more informed decision-making and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb439ae2-f28f-4d32-918a-337bc7ff7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Ans 04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33191b9-e768-4dcb-a9d0-825f56588f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' theorem and conditional probability are closely related concepts in probability theory.\n",
    "\n",
    "# Conditional probability refers to the probability of an event occurring given that another event has already occurred. It's denoted as \n",
    "# P(A∣B), where A is the event of interest and B is the condition.\n",
    "\n",
    "# Bayes' theorem provides a way to compute conditional probabilities in the reverse direction. It states that the conditional probability of\n",
    "# event A given event B can be calculated using the following formula:\n",
    "# P(A∣B)= (P(B∣A)×P(A))/P(B)\n",
    "# ​\n",
    "# Where:\n",
    "# P(A∣B) is the posterior probability of event A given evidence B.\n",
    "# P(B∣A) is the likelihood, the probability of observing the evidence B given that A is true.\n",
    "# P(A) is the prior probability of A being true before considering any evidence.\n",
    "# P(B) is the marginal likelihood of observing the evidence B.\n",
    "    \n",
    "# So, Bayes' theorem allows us to update our beliefs (the posterior probability) about the occurrence of an event A given new evidence B, taking\n",
    "# into account our prior beliefs (the prior probability) and the probability of observing the evidence under both scenarios. In essence, Bayes'\n",
    "# theorem provides a framework for reasoning about uncertain events and updating beliefs based on new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6333f6-e522-4914-8bb8-f08cf5a3a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Ans 05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a766bac-8478-415c-8ed1-b8848df992df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the appropriate type of Naive Bayes classifier depends on the characteristics of the dataset and the assumptions that\n",
    "# are reasonable for the problem at hand. There are several types of Naive Bayes classifiers, including Gaussian Naive Bayes, Multinomial\n",
    "# Naive Bayes, and Bernoulli Naive Bayes. Here's a guideline for choosing the type:\n",
    "\n",
    "# 1. Gaussian Naive Bayes:\n",
    "# a. Continuous Features: If the features in your dataset are continuous and assumed to have a Gaussian (normal) distribution, Gaussian Naive Bayes\n",
    "# is suitable.\n",
    "# b. Real-valued Features: When dealing with real-valued features like measurements or sensor data, Gaussian Naive Bayes is often a good choice.\n",
    "\n",
    "# 2. Multinomial Naive Bayes:\n",
    "# a. Categorical Features: When dealing with features that represent counts or frequencies of events (e.g., word counts in text classification),\n",
    "# Multinomial Naive Bayes is appropriate.\n",
    "# b. Text Classification: It's commonly used in natural language processing tasks such as document classification or spam filtering, where features\n",
    "# are typically represented as word counts or TF-IDF values.\n",
    "\n",
    "# 3. Bernoulli Naive Bayes:\n",
    "# a. Binary Features: If your features are binary-valued (e.g., presence or absence of a feature), Bernoulli Naive Bayes can be suitable.\n",
    "# b. Text Classification: Similar to Multinomial Naive Bayes, it's used in text classification tasks, particularly when representing features as\n",
    "# binary indicators.\n",
    "\n",
    "\n",
    "# In summary, the choice of Naive Bayes classifier depends on the nature of the features in your dataset. If your features are continuous and\n",
    "# normally distributed, Gaussian Naive Bayes may be appropriate. If your features are categorical or binary, Multinomial or Bernoulli Naive Bayes,\n",
    "# respectively, may be more suitable. It's often a good idea to try different types and evaluate their performance using cross-validation or other\n",
    "# validation techniques to determine the best approach for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbcfdeb-a989-46e1-93ab-d3b811725052",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Ans 06:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61317af-63b6-4bc2-a9b4-08e38fdf0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To classify the new instance with features X1=3 and X2=4 using Naive Bayes, we need to compute the posterior probability of\n",
    "# each class given these features and then choose the class with the highest probability.\n",
    "\n",
    "# Given that the prior probabilities for classes A and B are equal, we'll denote them as P(A) = P(B) = 0.5.\n",
    "\n",
    "# Now, let's compute the likelihoods for each class P(X1=3,X2=4∣A) and P(X1=3,X2=4∣B) using the provided frequency table:\n",
    "\n",
    "# For class A:\n",
    "# P(X1=3,X2=4∣A) = 4/23 * 3/23 = 0.02268\n",
    "\n",
    "# For class B:\n",
    "# P(X1=3,X2=4∣B) = 1/14 * 3/14 = 0.01530\n",
    "\n",
    "# Next, we need to compute the marginal likelihoods P(X1=3,X2=4) for normalization:\n",
    "# P(X1=3,X2=4) = P(X1=3,X2=4∣A) * P(A) + P(X1=3,X2=4∣B) * P(B)\n",
    "# P(X1=3,X2=4) = 0.02268 * 0.5 + 0.01530 * 0.5 \n",
    "# P(X1=3,X2=4) = 0.01899\n",
    "\n",
    "# Now, let's compute the posterior probabilities:\n",
    "\n",
    "# For class A:\n",
    "# P(A∣X1=3,X2=4) = (P(X1=3,X2=4∣A) × P(A)) / P(X1=3,X2=4)\n",
    "# P(A∣X1=3,X2=4) = (0.02268 * 0.5) / 0.01899\n",
    "# P(A∣X1=3,X2=4) ≈ 0.6\n",
    "\n",
    "# For class B:\n",
    "# P(B∣X1=3,X2=4) = (P(X1=3,X2=4∣B) × P(B)) / P(X1=3,X2=4)\n",
    "# P(B∣X1=3,X2=4) = (0.01530 * 0.5) / 0.01899\n",
    "# P(B∣X1=3,X2=4) ≈ 0.4\n",
    "\n",
    "# Since P(A∣X1=3,X2=4) > P(B∣X1=3,X2=4), Naive Bayes would predict the new instance to belong to class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57ce56ae-18b4-4060-9bd1-4974a5f69fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior probability of class A: 0.597105864432597\n",
      "Posterior probability of class B: 0.4028941355674029\n",
      "Naive Bayes predicts the new instance belongs to class A.\n"
     ]
    }
   ],
   "source": [
    "# Define the frequencies of each feature value for each class\n",
    "frequencies = {\n",
    "    'A': {'X1=1': 3, 'X1=2': 3, 'X1=3': 4, 'X2=1': 4, 'X2=2': 3, 'X2=3': 3, 'X2=4': 3},\n",
    "    'B': {'X1=1': 2, 'X1=2': 2, 'X1=3': 1, 'X2=1': 2, 'X2=2': 2, 'X2=3': 2, 'X2=4': 3}\n",
    "}\n",
    "\n",
    "# Compute the likelihoods for each class\n",
    "likelihood_A = (frequencies['A']['X1=3'] / sum(frequencies['A'].values())) * \\\n",
    "               (frequencies['A']['X2=4'] / sum(frequencies['A'].values()))\n",
    "\n",
    "likelihood_B = (frequencies['B']['X1=3'] / sum(frequencies['B'].values())) * \\\n",
    "               (frequencies['B']['X2=4'] / sum(frequencies['B'].values()))\n",
    "\n",
    "# Compute the prior probabilities\n",
    "prior_A = 0.5\n",
    "prior_B = 0.5\n",
    "\n",
    "# Compute the marginal likelihoods for normalization\n",
    "marginal_likelihood = likelihood_A * prior_A + likelihood_B * prior_B\n",
    "\n",
    "# Compute the posterior probabilities\n",
    "posterior_A = (likelihood_A * prior_A) / marginal_likelihood\n",
    "posterior_B = (likelihood_B * prior_B) / marginal_likelihood\n",
    "\n",
    "# Output the posterior probabilities\n",
    "print(\"Posterior probability of class A:\", posterior_A)\n",
    "print(\"Posterior probability of class B:\", posterior_B)\n",
    "\n",
    "# Make the classification decision\n",
    "if posterior_A > posterior_B:\n",
    "    print(\"Naive Bayes predicts the new instance belongs to class A.\")\n",
    "else:\n",
    "    print(\"Naive Bayes predicts the new instance belongs to class B.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9fb3707-d170-4c6d-a9aa-57c0702bb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
