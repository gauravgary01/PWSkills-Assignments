{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a82572f-ed0c-4283-80a7-fd2e49ad4aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f173b4-2785-42e0-aef8-0ccfa21d6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An ensemble technique in machine learning is a method that combines predictions from multiple individual models to produce a more accurate\n",
    "# and robust prediction. The idea behind ensemble methods is to leverage the diversity of different models to improve overall performance, much like how\n",
    "# a diverse group of experts might collectively make better decisions than any single expert alone.\n",
    "\n",
    "# Here are some common ensemble techniques:\n",
    "\n",
    "# 1. Bagging (Bootstrap Aggregating): This technique involves training multiple instances of the same base learning algorithm on different subsets of the training data,\n",
    "# often randomly sampled with replacement. The final prediction is typically the average (for regression) or majority vote (for classification) of the predictions made\n",
    "# by each model.\n",
    "\n",
    "# 2. Boosting: Boosting algorithms work sequentially, where each new model attempts to correct the errors made by the previous ones. Examples of boosting algorithms\n",
    "# include AdaBoost, Gradient Boosting Machines (GBM), and XGBoost.\n",
    "\n",
    "# 3. Random Forest: Random Forest is an ensemble learning method that combines multiple decision trees trained on different subsets of the data. Each tree is trained\n",
    "# independently, and the final prediction is the average (for regression) or majority vote (for classification) of the predictions made by individual trees.\n",
    "\n",
    "# 4. Stacking: Stacking (short for stacked generalization) involves training a meta-model that learns to combine the predictions of multiple base models. Instead of\n",
    "# using simple averaging or voting, stacking learns how to best weigh the predictions of each base model based on their performance on a holdout dataset.\n",
    "\n",
    "# Ensemble techniques are widely used in machine learning because they often lead to improved predictive performance and generalization ability compared to individual\n",
    "# models. They are particularly effective when the base models have different strengths and weaknesses, as the ensemble can compensate for individual model\n",
    "# shortcomings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26486c03-3c0e-477d-8859-cb8810b10dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab47a374-df86-46af-a1df-1112ed73cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "# 1. Improved Predictive Performance: Ensemble methods often yield better predictive performance compared to individual models. By combining multiple models,\n",
    "# ensemble techniques can mitigate the shortcomings of individual models and capture different aspects of the data, leading to more accurate and robust\n",
    "# predictions.\n",
    "\n",
    "# 2. Reduced Overfitting: Ensemble methods can help reduce overfitting, especially when individual models are prone to overfitting the training data. By averaging\n",
    "# or combining the predictions of multiple models, ensemble techniques can smooth out noise and variance in the data, leading to better generalization to unseen data.\n",
    "\n",
    "# 3. Increased Robustness: Ensembles are more robust to outliers and noisy data compared to single models. Since ensemble methods aggregate predictions from multiple\n",
    "# models, they are less likely to be affected by individual outliers or errors in the data.\n",
    "\n",
    "# 4. Capturing Complex Relationships: Ensemble techniques can capture complex relationships in the data by combining the strengths of different types of models. For\n",
    "# example, a combination of decision trees, linear models, and neural networks in an ensemble can collectively capture both linear and nonlinear patterns in the data.\n",
    "\n",
    "# 5. Handling Bias-Variance Tradeoff: Ensemble methods help balance the bias-variance tradeoff by leveraging the tradeoffs of individual models. For instance, a bagging\n",
    "# ensemble can reduce variance by averaging predictions from multiple high-variance models, while a boosting ensemble can reduce bias by sequentially correcting errors\n",
    "# made by previous models.\n",
    "\n",
    "# 6. Flexibility and Adaptability: Ensemble methods are flexible and can be applied to various types of models and datasets. They can be easily adapted and customized\n",
    "# to specific problem domains by selecting appropriate base models, ensemble architectures, and combination strategies.\n",
    "\n",
    "# Overall, ensemble techniques are widely used in machine learning because they offer a powerful approach to improving predictive performance, reducing overfitting,\n",
    "# increasing robustness, and handling complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e51e28-465f-4862-8d2c-6a06fceeebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8995c711-cc8d-4ab0-aaa7-0f9f31fef3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging, short for Bootstrap Aggregating, is an ensemble learning technique in machine learning where multiple instances of the same base\n",
    "# learning algorithm are trained on different subsets of the training data. It is used to reduce variance and improve the stability and accuracy of supervised\n",
    "# learning algorithms, especially decision trees.\n",
    "\n",
    "# Here's how bagging works:\n",
    "\n",
    "# 1. Bootstrap Sampling: Bagging begins by creating multiple bootstrap samples from the original training dataset. Bootstrap sampling involves randomly selecting\n",
    "# subsets of the training data with replacement, meaning that some samples may be selected multiple times while others may not be selected at all. Each bootstrap\n",
    "# sample typically has the same size as the original dataset but may contain duplicate instances.\n",
    "\n",
    "# 2. Training Multiple Models: After creating bootstrap samples, multiple instances of the base learning algorithm (e.g., decision trees) are trained independently\n",
    "# on each bootstrap sample. Since each model is trained on a slightly different subset of the data due to bootstrap sampling, they are likely to produce different\n",
    "# predictions.\n",
    "\n",
    "# 3. Aggregating Predictions: Once all models are trained, predictions are made for new unseen data by aggregating the predictions of individual models. For regression\n",
    "# tasks, the final prediction is often the average of predictions made by each model. For classification tasks, the final prediction is typically determined by\n",
    "# majority voting among the predictions of individual models.\n",
    "\n",
    "# Key characteristics of bagging include:\n",
    "\n",
    "# 1. Reduction of Variance: By training multiple models on different subsets of the data and averaging their predictions, bagging helps reduce the variance of the final\n",
    "# prediction. This can lead to improved generalization performance, especially when the base models tend to overfit the training data.\n",
    "\n",
    "# 2. Stability and Robustness: Bagging improves the stability and robustness of the learning algorithm by reducing the impact of outliers and noise in the training data.\n",
    "# Since each model is trained on a subset of the data, they may focus on different aspects of the data, leading to more robust predictions.\n",
    "\n",
    "# 3. Parallelization: Bagging can be easily parallelized because each model is trained independently on a separate subset of the data. This makes bagging particularly\n",
    "# suitable for distributed computing environments and parallel processing architectures.\n",
    "\n",
    "# Common implementations of bagging include Random Forest for decision trees and Bagged Trees for other types of base learners. Overall, bagging is a powerful technique\n",
    "# for improving the performance and stability of supervised learning algorithms, especially when dealing with high-dimensional or noisy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ef5f42-56b6-4ae9-8739-1d5848ea08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0d9650-c16d-4fc9-aa30-e53b2102ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting is another ensemble learning technique in machine learning that combines multiple weak learners (often decision trees or other\n",
    "# simple models) to create a strong learner. Unlike bagging, where models are trained independently in parallel, boosting trains models sequentially, with each\n",
    "# new model attempting to correct the errors made by the previous ones.\n",
    "\n",
    "# Here's how boosting typically works:\n",
    "\n",
    "# 1. Sequential Training: Boosting starts by training a base learner (weak model) on the original dataset. This base learner could be a simple model, such as a\n",
    "# shallow decision tree.\n",
    "\n",
    "# 2. Weighted Data: After the first model is trained, the algorithm assigns weights to the training instances based on their performance. Instances that were\n",
    "# misclassified by the first model are given higher weights, while correctly classified instances are given lower weights.\n",
    "\n",
    "# 3. Iterative Training: In subsequent iterations, the algorithm focuses more on the instances that were misclassified in the previous iteration. It trains a new\n",
    "# base learner on a modified version of the training dataset, where the weights of misclassified instances are increased.\n",
    "\n",
    "# 4. Weighted Aggregation: Once all models are trained, their predictions are combined using a weighted sum, where models that perform better on the training data\n",
    "# are given higher weights. The final prediction is often determined by a weighted sum of the predictions made by each model.\n",
    "\n",
    "# Key characteristics of boosting include:\n",
    "\n",
    "# 1. Emphasis on Misclassified Instances: Boosting focuses on instances that are difficult to classify, gradually improving the model's performance by giving more\n",
    "# attention to these instances in subsequent iterations.\n",
    "\n",
    "# 2. Sequential Learning: Boosting trains models sequentially, with each new model attempting to correct the errors made by the previous ones. This iterative process\n",
    "# helps boost the overall performance of the ensemble.\n",
    "\n",
    "# 3. Model Complexity: Boosting can create complex models by combining multiple weak learners, leading to high predictive performance. However, this complexity can\n",
    "# also increase the risk of overfitting, so careful tuning of hyperparameters is important.\n",
    "\n",
    "# 4. Common implementations of boosting include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), XGBoost, and LightGBM. These algorithms differ in\n",
    "# their specific strategies for assigning weights to instances and training subsequent models.\n",
    "\n",
    "# Overall, boosting is a powerful technique for creating accurate and robust predictive models, especially when dealing with classification and regression tasks on\n",
    "# complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8016ad-d17f-4c40-aedd-10bffc6f1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e87430-321d-4b86-96dd-de99a4cba769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble techniques offer several benefits in machine learning:\n",
    "\n",
    "# 1. Improved Predictive Performance: Ensemble methods often yield better predictive performance compared to individual models by combining the strengths of multiple\n",
    "# models. This can result in more accurate and robust predictions, especially when individual models have different strengths and weaknesses.\n",
    "\n",
    "# 2. Reduction of Overfitting: Ensemble techniques can help reduce overfitting, particularly when base models are prone to memorizing noise in the training data. By\n",
    "# aggregating predictions from multiple models, ensemble methods can smooth out noise and variance, leading to better generalization to unseen data.\n",
    "\n",
    "# 3. Increased Robustness: Ensembles are more robust to outliers and noisy data compared to single models. Since ensemble methods aggregate predictions from multiple\n",
    "# models, they are less likely to be affected by individual outliers or errors in the data, resulting in more stable and reliable predictions.\n",
    "\n",
    "# 4. Handling Complex Relationships: Ensemble techniques can capture complex relationships in the data by combining the strengths of different types of models. For\n",
    "# example, a combination of decision trees, linear models, and neural networks in an ensemble can collectively capture both linear and nonlinear patterns in the data,\n",
    "# leading to improved model performance.\n",
    "\n",
    "# 5. Balancing Bias-Variance Tradeoff: Ensemble methods help balance the bias-variance tradeoff by leveraging the tradeoffs of individual models. For instance, a bagging\n",
    "# ensemble can reduce variance by averaging predictions from multiple high-variance models, while a boosting ensemble can reduce bias by sequentially correcting\n",
    "# errors made by previous models.\n",
    "\n",
    "# 6. Flexibility and Adaptability: Ensemble methods are flexible and can be applied to various types of models and datasets. They can be easily adapted and customized to\n",
    "# specific problem domains by selecting appropriate base models, ensemble architectures, and combination strategies, making them suitable for a wide range of machine\n",
    "# learning tasks.\n",
    "\n",
    "# Overall, ensemble techniques are powerful tools in the machine learning toolbox, offering improved predictive performance, robustness, and flexibility across various\n",
    "# domains and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce12e66-c368-40a3-9213-66596d4928f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 06:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b741e635-5176-4e5d-9df6-5bea16cfca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble techniques are powerful tools in machine learning, but whether they are always better than individual models depends on various\n",
    "# factors, including the specific problem, dataset characteristics, and the quality of individual models. Here are some considerations:\n",
    "\n",
    "# Quality of Base Models: The effectiveness of ensemble techniques heavily relies on the quality of the base models they combine. If the base models are weak or\n",
    "# highly correlated, the ensemble may not yield significant improvements over individual models. In such cases, it's essential to ensure that the base models are\n",
    "# diverse and complementary to each other.\n",
    "\n",
    "# Dataset Characteristics: The performance of ensemble techniques can vary depending on the characteristics of the dataset. In some cases, when the dataset is small\n",
    "# or contains highly noisy data, ensemble methods may overfit or perform poorly. Additionally, if the dataset is inherently simple and can be effectively modeled by\n",
    "# a single model, ensemble techniques may not provide significant benefits.\n",
    "\n",
    "# Computational Resources: Ensemble techniques typically require more computational resources compared to training individual models. If computational resources are \n",
    "# imited, training and deploying ensemble models may not be feasible or practical, especially for real-time applications or resource-constrained environments.\n",
    "\n",
    "# Interpretability: Ensemble models are often more complex and less interpretable compared to individual models. While they may offer improved predictive performance,\n",
    "# the trade-off is often a loss of interpretability, making it challenging to understand the underlying relationships in the data and interpret model decisions.\n",
    "\n",
    "# Risk of Overfitting: Ensemble methods, particularly those based on boosting, can be susceptible to overfitting if not properly tuned or regularized. Sequentially\n",
    "# training models to correct errors made by previous models may lead to memorizing noise in the training data, resulting in overfitting and poor generalization to\n",
    "# unseen data.\n",
    "\n",
    "# In summary, while ensemble techniques can be highly effective in improving predictive performance and robustness, they are not always guaranteed to outperform\n",
    "# individual models. It's essential to carefully consider the characteristics of the problem, dataset, and available resources before deciding whether to use ensemble\n",
    "# techniques or rely on individual models. Experimentation and empirical validation are often necessary to determine the best approach for a particular machine learning\n",
    "# task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f781801-6bf8-4564-aa29-49f17cd2b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 07:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6fdf95-36fc-431f-8a10-d3dd8418a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confidence interval calculated using bootstrap resampling involves the following steps:\n",
    "\n",
    "# 1. Bootstrap Resampling: Generate multiple bootstrap samples by randomly sampling with replacement from the original dataset. Each bootstrap sample should have the\n",
    "# same size as the original dataset.\n",
    "\n",
    "# 2. Compute Statistic: Calculate the statistic of interest (e.g., mean, median, standard deviation, etc.) for each bootstrap sample. This statistic could be anything\n",
    "# that summarizes the data distribution or model performance.\n",
    "\n",
    "# 3. Calculate Confidence Interval: Use the distribution of the statistic calculated from the bootstrap samples to estimate the confidence interval. The confidence\n",
    "# interval represents a range of values within which the true parameter (e.g., population mean) is likely to lie with a certain level of confidence.\n",
    "    \n",
    "# a. Determine the desired confidence level (e.g., 95%, 99%).\n",
    "# b. Sort the bootstrap statistics in ascending order.\n",
    "# c. Calculate the percentile-based confidence interval based on the desired confidence level. For example, for a 95% confidence interval, you would typically use\n",
    "# the 2.5th and 97.5th percentiles of the bootstrap statistics.\n",
    "\n",
    "# Here's a Python code example demonstrating how to calculate a confidence interval using bootstrap resampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c56d1dc-c77c-4a37-985e-3b6c20fe5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95.0%):\n",
      "Lower Bound: 3.7\n",
      "Upper Bound: 7.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset or sample\n",
    "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Function to compute the statistic of interest (e.g., mean)\n",
    "def compute_statistic(data):\n",
    "    return np.mean(data)\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_statistics = []\n",
    "for _ in range(num_samples):\n",
    "    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    bootstrap_statistic = compute_statistic(bootstrap_sample)\n",
    "    bootstrap_statistics.append(bootstrap_statistic)\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_level = 0.95\n",
    "lower_percentile = (1 - confidence_level) / 2 * 100\n",
    "upper_percentile = (confidence_level + (1 - confidence_level) / 2) * 100\n",
    "confidence_interval = np.percentile(bootstrap_statistics, [lower_percentile, upper_percentile])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval ({}%):\".format(confidence_level * 100))\n",
    "print(\"Lower Bound:\", confidence_interval[0])\n",
    "print(\"Upper Bound:\", confidence_interval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5bc0418-8e89-4974-8f08-aa77a03f920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 08:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8b37fe3-b0ad-4a70-80fa-9c1c7dcda64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic or to quantify the uncertainty\n",
    "# associated with a sample estimate. It involves generating multiple random samples (bootstrap samples) from the original dataset by sampling with replacement.\n",
    "# These bootstrap samples are then used to calculate statistics of interest or to estimate confidence intervals.\n",
    "\n",
    "# Here are the steps involved in bootstrap resampling:\n",
    "\n",
    "# 1. Original Dataset: Start with a dataset containing observed data or samples.\n",
    "\n",
    "# 2. Bootstrap Sampling: Generate multiple bootstrap samples by randomly selecting observations from the original dataset with replacement. Each bootstrap sample\n",
    "# has the same size as the original dataset, but individual observations may be selected multiple times, while others may not be selected at all.\n",
    "\n",
    "# 3. Statistic Calculation: Calculate the statistic of interest (e.g., mean, median, standard deviation, regression coefficient, etc.) for each bootstrap sample.\n",
    "# This statistic serves as an estimate of the parameter or quantity of interest based on the bootstrap sample.\n",
    "\n",
    "# 4. Estimate Sampling Distribution: Use the statistics calculated from the bootstrap samples to estimate the sampling distribution of the statistic. This distribution\n",
    "# provides information about the variability of the statistic and can be used to quantify uncertainty.\n",
    "\n",
    "# 5. Confidence Interval Estimation: Construct confidence intervals based on the estimated sampling distribution. Confidence intervals provide a range of values within\n",
    "# which the true parameter (e.g., population mean, median, etc.) is likely to lie with a certain level of confidence.\n",
    "\n",
    "# Bootstrap resampling allows for the estimation of standard errors, confidence intervals, and hypothesis testing without relying on theoretical assumptions about the\n",
    "# underlying distribution of the data. It is particularly useful when the sample size is small, the data distribution is unknown or non-normal, or when traditional\n",
    "# statistical methods are not applicable.\n",
    "\n",
    "# Here's a brief summary of the steps involved in bootstrap resampling:\n",
    "\n",
    "# 1. Start with the original dataset.\n",
    "# 2. Generate multiple bootstrap samples by randomly sampling with replacement from the original dataset.\n",
    "# 3. Calculate the statistic of interest for each bootstrap sample.\n",
    "# 4. Estimate the sampling distribution of the statistic using the bootstrap statistics.\n",
    "# 5. Construct confidence intervals or perform hypothesis testing based on the estimated sampling distribution.\n",
    "\n",
    "# By repeating these steps multiple times, bootstrap resampling provides a robust and computationally efficient method for estimating uncertainty and making statistical\n",
    "# inferences from observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a516efb4-cd33-4b13-917b-17cebf824942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Ans 09:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "881d3ca7-5256-4665-9694-f87a96815246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95%):\n",
      "Lower Bound: 14.498105061036055\n",
      "Upper Bound: 15.57206055663977\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample data (sample size = 50)\n",
    "sample_mean = 15  # meters\n",
    "sample_std = 2  # meters\n",
    "sample_size = 50\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstraps = 1000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_means = []\n",
    "for _ in range(num_bootstraps):\n",
    "    # Generate bootstrap sample by sampling with replacement\n",
    "    bootstrap_sample = np.random.normal(loc=sample_mean, scale=sample_std, size=sample_size)\n",
    "    # Calculate mean height for the bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval (95%):\")\n",
    "print(\"Lower Bound:\", confidence_interval[0])\n",
    "print(\"Upper Bound:\", confidence_interval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "403fe0e2-cd98-4337-8fb5-5aa2c9f7ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
